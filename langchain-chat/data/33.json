[
    {
        "content": "This interview is recorded shortly after publishing our integration with COHERE's multilingual embedding models and Weaviate to try this out for yourself, please check out this excellent blog post written by Sebastian Witalec. This is linked in the description of the video. I really hope you enjoy this interview with Nils Reimers. I learned so much as the interviewer, and I really hope you do as well. As always, we'd be more than happy to answer any questions or discuss any ideas you have. Thanks so much for watching. Hey everyone, I'm so excited about our next Weaviate podcast. We're hosting Nils Reimers Nils Rimers is one of the most influential scientists in this whole area of deep learning for search. He's done so much incredible pioneering work, uh, especially with text search. And I'll kinda hop into this, uh, the narrative as I see it, and. Uh, ask Nils to kind of hop in on the origin of this kind of research, but, uh, to me it kind of looked like we started with this thing where we could extract representations of data from the intermediate layers of deep learning models. And then what happened is we started optimizing directly for those representations using contrastive learning loss functions. And I think this was super successful in computer vision with papers like SimCLR, MoCo. And then Nils and his team, started showing how we could do this with, uh, Siamese BERT Networks, Sentence BERT, and, uh, developed the whole sentence, transformers library, the BEIR benchmarks, and really showing how successful this technique could be. So, uh, firstly, Nils, thank you so much for joining the podcast.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yes. Pleasure to be here.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Uh, so could you kind of take it on that, uh,  kind of what I left off. Uh, could you kind of take over the, like, kind of the origin story of how you came to be working on, this kind of like Siamese encoding of representation?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yeah, sure. It was not really planned that I go into that field. So back in 2018, I had been working on clustering arguments. Our research group was interested in controversial topics like whether or not nuclear power plants should still be run. We wanted to cluster arguments for and against this topic. At that time, methods like the Universal Sentence Encoder and ENT were available. However, the big problem was that these models were only released by companies like Facebook and Google, but no training data or information on how to train the models was provided. This was frustrating for me because I really wanted to have models that I could train specifically for this task. This was the start of my journey to figure out how to train embedding models and I thought, okay, let's start a library that makes it extremely simple to train your own embedding models. At that time, BERT had recently been published, so I thought, okay, let's figure out how to use BERT for clustering information. This was the origin of Sentence-Transformers and since 2018, I've been hooked on this topic because representing data in vector spaces opened up so many opportunities and cool applications you can build with it.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "That's so interesting. Uh, yeah, I learned a lot about topic analysis from talking with, uh, Martin Grootendorst on our other podcast about BERTopic. I know we also came on, uh, cohere Talks and talked with Jay and, uh, yeah, that, that topic analysis, such an interesting point to it. Um, so maybe, uh, coming into it, um, so can we start out with, So learning representations of texts with deep learning. Um, I'm really curious about if you could kind of tell this whole story, sort of like, I know about like the whole in batch negatives, the, um, can you kind of tell the story around just like your investigations into sort of how you sample positive pairs and negative pairs, model size, data size, training requirements, all these kind of.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yes. Uh, so the original sentenceBERT paper followed the, the approach from InferSent using NLI data and training like cross encoders, which I totally can't recommend to do anymore. So, so actually the original first sentence per model performed really poorly. Um, even such, the benchmarks were really good at that time, or the, the benchmark scores were good if you really apply them to a bit more complex data. Um, they performed really poorly. They also performed really poorly, worse than universal sentencing encoder at that time. So this was like really nagging for me, like, okay, how can you train better models and how can you evaluate better models? So this is like two major pillars of my research, not be justified. Hey, I got bold numbers on some arbitrary benchmark because most benchmarks are pretty bad. But first, how can you assess? They're really good and how can you make them better? The InfoNCE loss when I started was bit, it existed, but it was like really, really unknown. So the universal sentence encoder method they trained already was InfoNCE loss, or, I mean, there are so many names by it. Um, but it was not really described in the paper, so it was like really hidden. And then if you follow up some, some other papers, they had like really complicated formulations for InfoNCE loss. . Um, and yeah, no, no training code was available. But yeah, at some point, um, with the help of a student help of mine, uh, we actually implemented one of the really old implementations of the in loss from Google, and I shared like really good results. And then it also made 'em a lot more sense than the previous cross entropy classification loss. Big things which are relevant here is like batch size. So that's a simple trick to increase your, your embedding performance, increase the batch size. But sadly, at some point you run into limitations of GPU memory. And because we don't have like an arbitrary number of GPU memories in my research lab back then, um, we invest a lot of time how to make the batches better, specifically adding hard negatives test to it. But here you need a lot of cleaning cells, so really you want to push. Anchor and positive close in the vector space and anchor negative distant in the vector space. But this loss is extremely sensitive if the data's unclean. So if anchor and positive, it's not really positive pair, that's extremely hurting the performance. Similar when anchor and negative when it's not really negative, but a positive, another positive that's also extremely hurting, um, the performance of the model. So spend a lot of time thinking, working, testing, how can you make it nice and clean and have like really high quality data at scale.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Uh, it's so interesting. And, uh, another kind of podcast we did was with Ori Ram on, uh, the spider algorithm, learning to retrieve without supervision, how, um, you look for overlapping terms to form the positives. And as you mentioned, the noise and the negatives. I've always been so interested in that kind of thing. So what, so what are you thinking currently about the positive negative sampling scheme? Is it, uh, you know, like just adjacent paragraphs are positives, and then if you do that at scale, it kind of like makes it all right.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "What I'm a big fan of is like to use more powerful, so-called cross encoder model for data cleaning. So often you start with like some anchor positive pairs. These, these are often quite easy to, to get. You take scientific publications and you'll say, okay, the title and the abstract, that's probably like a good pair that should be close in the vector space. Or you go on, um, stack exchange and you take a question and you take the highest ranked answer and you say, okay, that's also good pair. That should be close in the vector space. But how do you get like really hard negatives? So hot negative is for example, I dunno. For example, you have a question, how can I sort a python list in descending order? And then the positive would be how to do it. And the hardest negative would be to say, okay, this is how you do it, but in ascending order. So there's like a slightly tiny detail, everything else matches, but there's a slightly tiny detail. That makes it not a valid answer. And finding this is like really, really hard. So how can you find like a negative that's so close to the correct answer, but it's still still negative and often you overshoot so you get like the, the a negative. That's also where person would say, yeah, that's another positive. Um, so here I really like cross encoders. Cross encoders are a lot more powerful than bi encoders. Um, to, so, so you first train a cross encoder and then the cross encoder can do cross attention between the query and all the, the candidates and can really see at like these fine details. Like, okay, is it like really matching all the aspects you're asking in your question. And then at the end give you like a score and then you use this cross encoder. Uh, you go over all your triplets, black anchor positive, negative. And say, okay, here we have like, actually negative pairs, uh, or negative candidates.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
       "content": "Yeah. It's, uh, I think there's a, there's a bit of that that I wanna unpack and I think, um, you know, being here talking to Nills Reimes, I wanna just dive right into the, the details of these technical questions. Um, But, so maybe for our listeners quickly, the cross encoder is this idea where you take the query and the document as input to like a high capacity transformer that will output a score and it's slower than the bi encoders, but it's like super accurate because of the high capacity of the transformer. Uh, so I want to come back into the, the, you know, you mentioned the stack exchange and I think the work on the 1 billion training pairs and then the diversity of the BEIR benchmarks. I think those are so interesting. I really wanna come back to these topics, but quickly, again, talking to Nils Reimers, I have to ask this question. What about like a knowledge distillation from the cross encoder to the bi encoder where the cross encoder, uh, it kind of sounds like that's what you're getting at, where it's filtering the data. Could it, you know, have like a soft label where the score that comes outta the cross encoder is the, um, uh, what the dot product should be?",
       "sender": "Connor Shorten",
       "channelNum": 33
    },{
        "content": "Yeah. Um, yeah. I'm super big fan of this. So Sebastian Hofstatter established this in a really cool work where he showed how you can distill knowledge from cross encoders to bi-encoders. Which maps to the margin MSE loss, which solves a lot of these is issues. So in contrastive training, you need the positive to be really positive to the query, and the negative must be really negative to the query. So you spend a lot of time cleaning and trying to get like the hardest possible negative that is still a negative and not yet a positive. But with margin MSE loss, you take the triplet query positive and some other candidates, you pass it through a cross encoder to get like estimates from the cross encoder, how close are the two candidates to the query? And then you transfer this knowledge to a bi-encoder. And this totally eliminates the issue of getting really clean data. So you can run it with like really dirty data, which is nice. You can run it with like really, really hard negative so far with. If you trained buying coders and the negative is like too hard for when bi-encoder. That extremely hurts your performance, but now you can still still run it. So I'm a big fan, so I, in most cases I moved away from contrastive training to margin MSE training. Downside here is a bit more overhead, so contrastive training there, it's like really easy to get training examples, you go on Stack Exchange, You download it, you get the question, you get like 400 million questions, the highest rank answer, so you get directly a hundred million uh, pairs you can use from contrastive training, but with margin MSE loss you have to do negative mining. You have to have good cross encoder. You have to take the cross encoder to score all Query anchor, uh, query positive negative triplets. So there's like a lot of overhead involved in that.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Sorry to ask a clarifying question, but with the margin MSE, that's where don't you have to kind of explicitly set the margin in the triplet loss? And I always thought for that reason it was sort of funky. Like, because it's kind of like a alpha minus max, uh, kind of thing, right?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "So in the margin MSE loss, you take the cross encoder, you ask like two questions like query and candidate one, and query candidate take two. What's the relevance? And this gives you like two relevance. So the one has, for example, I dunno, relevance 10, the other has relevance five. And then you use this distance. So you say, okay, the, the, the distance from candidate eight to candidate B is like,  and then you transform this to a bi encoder that the bi encoder, if you compute the dot product between query candidate A and candidate B should also be five in the vector space. So basically what you do is you arbitrarily pick three points. So you have the query, you have candidate A, candidate B, you ask the powerful cross encoder, what's the distance? Uh, and then you teach the buying coder. Match also the distance in the vector space. So the crossing coder teaches the the buying coder about distances in the vector space.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Oh, okay. Fantastic. Yeah. Awesome. So yeah, I think that idea is gonna be super powerful with the, because you the cross encoder is kind of slow, especially if you're scaling it to a billion documents and then maybe you wanna retrieve a thousand, re-rank a thousand. So I think that kind of distillation will be super impactful. Can you tell me about, I wanna dive into the effort on collecting the billion data pairs. I think it's like Wikipedia, StackExchange, Reddit, like a big collection. Can you tell me about the effort involved in that?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yeah. Last year at HuggingFace we had like this community event, where we got some resources sponsored by Google Cloud to work on TPU and Jax. And then we said, okay, it would be cool to really train embedding models on a lot of training data so far our SentenceBERT model had been trained on rather small training sets. Universal Sentence Encoder had been trained on like billions of training pairs, but sadly training was not available. So we put the efforts together and found like, okay, what are good resources, good labeled resources. One is like, I don't know, MS MARCO or NQ, all these datasets, Quora duplicate questions, to just collect them, bring them to a standard format. And then Stack Exchange have a lot of data available, but in XML format. So it took some preprocessing time to go from XML down to high quality Q and A pairs. Similar for Reddit. There are like some big dumps available on Reddit, so took some time to process it, to get like, comment and answer pairs, Post and comment structures. And then, yeah, so spent a lot of time to get the data, clean the data, and then train the model. So in general, big advocate on data quality. So people should spend a lot of time to get like high quality data. Then truly the models you apply and the methods you apply is like rather straightforward after.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Yeah, it's really interesting. Because the contrastive learning is self supervised, like the auto-regressive, predict the masked token. And so I think maybe the focus on data quality is less so because the philosophy is like, we're taking advantage of all this unlabeled data. So can you maybe tell me a little more about how you think about data quality in the self supervised thing where you have just the crazy amount of data? So how do you clean it? Do you duplicate it?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yeah, so in general what we see is that data quality is critical, like in every aspect, be it a generative model or an embedding model. What works nice is often a two-stage approach. First, you you train with some approach on noisy data. You take a BERT network and you run masked language modeling on a lot of data. You run like this inverse cloze task on a lot, like, a lot of data. So where you take a paragraph and randomly take a sentence from this paragraph and then this is your pair. But what's really critical is, um, the second stage where you train like on high quality data for the task you want. Here, data quality plays like an extremely critical role. So out of the box BERT with MLM or ICT training doesn't work well for search. But if you then show like some search examples you can get really good, strong results. And yeah, I think it's valid. You can focus on both. So first pre-training or for more on the fine tuning step. I don't have an intuition yet on, which is more important. Should you more focus on the pre-training or should you more focus on the fine tuning? I would say, me personally, I've spent more time on fine tuning and tried to find high quality question, answer pairs or triplets to fine tune the model on this.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Yeah. It's so interesting and I definitely wanna dive into the new Cohere multilingual model and see what details I can tease out of you. But with the sentence transformer, all-mini-lm, that's been a super impactful model. t has an incredible zero shot kind of ability. Is that the, that's the pre-trained model? Correct. And it hasn't been fine tuned yet? So it's like the pre-train on massive data?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "No, the all-mini-LM model has been pre-trained on these like billion pairs, like question, answer pairs, duplicate questions, and so on.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Very interesting. And yeah, I think the zero shot ability of those models to encode all these domains, I think that's just a huge thing. In our last podcast, Chris Dossman had said that a lot of these tools are like the perfect mvp, minimum viable product tool. That it solves like 80% of the cases out of the box. And I think it's very interesting, as we talk about this kind of philosophy of fine tuning, so I think this would be a great topic to kind pivot into the philosophy of Cohere's models and as you're developing these super powerful models, how you're thinking about pre-training and fine-tuning, generally anything you want to talk about.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Sure. So yeah, at Cohere we recently launched, our multilingual embedding model this week Monday. So far we have these really large dataset collections in English. So, because in English it's easy, you go on Stack Exchange, you download the XML files. And then you have to do the hard work to clean or extract data from the XML files, but at least you have the big dump of stack exchange. And this gives you already like a hundred million question, answer pairs. So if you train a model on this, it performs extremely good. So far an issue with multilingual models has been data. Some people previously used like machine translation to translate MS MARCO to other languages, but here the issue with multilingual models is that they learn a US-centric machine translated view on topics. So for example, in MS MARCO, a big topic is like, how do I do my taxes in the US? So if I do this and train it with machine translation translated to German, the model learns in German, how do I file my taxes in the US? and how do I file certain forms? But it has like a blind spot. How do I do my taxes in Germany? And me as a German sender, I want to do know how do I do taxes in Germany, and how do I fill in like certain tax forms in Germany? And so this was like a big missing spot of a lot of previous models. So most previous models only worked on a sentence level, which is really bad for search, where you often want to match a query to a long paragraph or longer document. So here we took the same recipe as described before. So we first collected a lot of data from the web, so found like a lot of resources, like tens of thousands of these websites, communities, FAQ pages on websites, news articles, and so on. And then try to find high quality training pairs in here, question, answer pairs, basically. Did a lot of filtering to really filter out high quality question answer pairs. Here, a big challenge is to scale to a lot of languages, so in English, there's like a way how you can look at, okay, what's a good pair? What's a bad pair? You can do like some reg x, some filtering. I could, for example, I dunno, count the number of white spaces to see if a paragraph is well formatted, or it's just like some random string. But the challenge is if you do it across hundred languages, it's like so diverse. So for example, if you go to to Chinese. You don't have like white spaces. So how do you know if a text is actual proper Chinese text, or is it just some random Chinese words? And so somehow you have to find like ways to scale to these other languages. And then the third step was data augmentation, finding good hard negatives, cleaning them, and then taking triplets and training the embedding model. And yeah, overall we got like 900 million English triplets, close to 500 million non-English triplets. And then passing this to a model which gives you, again, a model that has seen a lot of topics. You can apply it to your taxes or online gaming or beauty or fashion or computer science. Because likely in these 500 million non-English questions, it has seen such a topic or similar topic already.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "It's so interesting. There's so many things I wanna unpack. I mean, yeah, the first I think is, there's also this MIRACL benchmark that's going around on multilingual, and I think that's about, I'm trying to guess now, I don't think quite 500 million if you sum total all the languages other than English. And I know it's 32 million English, so it sounds like a massive dataset. This is kind of the thing I wanted to get a little more into with this kind of private models, public models thing is, I think with the private models and the kind of business models around it, it makes sense that, Cohere, or say OpenAI on the other side of the fence, can build these like massive datasets and be incentivize to build it and have a sustainable thing around building 900 million English, 500 million non-English pairs. So maybe could you talk more about just like the challenges of collecting data when building the Sentence Transformers project compared to now at Cohere?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "I mean the challenges are already similar. You have to do work, which is for a lot of people painful. So a lot of people don't enjoy working on data. So people wanna find the best loss function and try to, I dunno, where, add some skip connections to it. But people do not really enjoy working on data like, download this or find like all these XML files from Stack Exchange, for example, open the Stack Exchange format, parse it, find the right pairs on this. So, so people do not really enjoy this, but I think that's where a lot of value comes from. Mm-hmm. . Um, obviously if you process a lot of data, it also requires a lot of compute, so that's that. True. That's kind of been neglected. So if you have like a billion training pairs and you wanna train on this, this takes a lot of time. But also data cleaning. So we pass like these billion training pairs through a cross encoder. So first you have a big cross encoder and then you need to do inference on a billion pairs. So there's also a significant amount of time.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Yeah, well that's super interesting. Yeah, there's that whole thing is really interesting. I want to kind of come back into the multilingual thing and something that's always scared me with multilingual, is kind of as you say about the Chinese, you don't know when the white space or say the period heuristic or line break for paragraph heuristics. All that is so useful. And I'm also jealous of say the Europeans. Um, I know at SeMI say, Bob and Etienne, and they all speak multiple languages, whereas me, I only speak English. So like with the multilingual, what was your interest in diving in? Do you have any hesitancy on maybe the domain specific thing, like debugging your Swahili model? Right. It would be a little difficult.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
       "content": "Yeah that's definitely hard to debug this. First I would say there's a big need for this. So me as a German sender, I mean, it's always nice if I go to English, there are like so many models and systems available. But even if you go to German and German, I mean, it's still like a high resource language, has a decent amount of population, it's quite powerful economically, but still the number of things you can get is like really limited and it's like really frustrating to see that in English, you can build these amazing semantic search systems, which works extremely good for Q and A retrieval. But even if you go to German, there's like hardly any models available. And then if you go further down the the language layer to like more and more low resource languages, it's getting harder and harder to build this. Second motivation is building Multilingual search with lexical search is like really painful. So if you build this in elastic search, for example, you need to know the language. So you need to know the language of the document, and for every language you have a different index. Because every language requests a different tokenizer, requires a different stop word list, requires, a different stemmer. So at the end, let's say you want to target, I dunno, European Union with, I dunno, 20 languages. You have like 20 different indices in elastic search. Each has a different tokenizer, stemmer, stop word list. . For each document you have to run language identification to see, is it a German document or a French document? And then the big challenge comes at query time. Is it like a German query or a French query or an English query? For some really short queries, it's unclear. It can be English and German or French and Spanish. And so how do you query these different indices? Because a Spanish query, you have to add a Spanish index. But with dense retrieval it is extremely trivial. So you can have just one index. You take your text, you pass it through the embedding model. This gives you some embeddings. You pass it to your dense, to your vector db. You don't have to do any language identification. Don't have to build different indices. Don't have to do different stemming and stop word list. So it's like super easy to build like dense retrieval, for a 100 languages, which is like completely painful and impossible with Lexical search.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "That's super interesting. I thought the last I saw strong hybrid search results on multilingual. I'm really not, yeah, it sounds like the whole separate stop words problem. I don't know what the stop words are in Korean, so I see how that kind of shift sounds really challenging. I kind of want to pivot into sort of an idea I've just seen recently and get your opinion on it. This thing was called, this paper was called, Task Aware Retrieval with Instructions, where sort of the interface is, in addition to that search box, you also explicitly describe your intent and then you have like an embedding of the intent that you would use to re-rank the candidates in a vector search way as well. So what do you think about that kinda idea? Because you mentioned the problem. Okay, I got to understand if this query is Korean or, uh, what's the neighboring language? I'm not a great language guy, but like this idea of you explicitly describe your intent as well.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yeah, so in the BEIR benchmark we have one data set, which is really interesting. It's called ArguAna. When you want to find counter-arguments, so you have an argument, say, nuclear energy is super safe, and then you want to have retrieval to find counter-arguments to say, okay no, nuclear energy is not safe. Obviously, out of the box models, if I search for, nuclear energy is safe, it finds like different arguments that also mentioned nuclear energy is one of the safest energy sources, so there are the questions like, how can I tell the model of my intent that I don't want to have arguments that are similar, but arguments that are opposing? And the paper you mentioned, use this in terms of kind of like instruct style. Say, okay, find a counter argument, nuclear energy is safe, or find a similar argument, or find evidence. I think it's a nice idea, especially if people want to build this into their product. So if you have like a search engine and you wanna find some arguments with supporting and opposing evidences and different perspective, so I think it's an easy way for machine learning engineers and search engine engineers just to prepare the prefix and say, okay, now I wanna search for opposing documents or, supporting evidence or opposing evidence for this.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "There's a few, I kind of want to stay on this idea of, well, I think we could go into like the kind of the prompting and ChatGPT and that idea of instructions and intent and the interface around it. But I kinda want to stay on this like multi-vector search, like vector re-ranking. Have you had a chance to see the ColBERT idea and do you have any thoughts on it? Just kind of, you keep token representations to re-rank in a vector search way?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yes, I'm quite familiar with ColBERT. So yeah, ColBERT is a nice idea. So far the challenge with ColBERT is deployment costs. So the idea of ColBERT is you take a text and you don't encode it to a single embedding, but you take every token and you embed it to an individual embedding. This gives you kind of like an explosion of embedding. So if you take a paragraph with a hundred tokens, it doesn't result in a single embedding, but to a hundred embeddings. And this down the line lets the cost for the vector DB to to be exploded. So because the cost of vector DB is like a hundred times higher and this is really significant. So if you wanna do semantic search vector search on English, Wikipedia, It's not too big. It's like, I dunno, 5 million articles, but you pay like roughly $2,000 a month to run an AWS instance on this. If this is like a hundred times higher, so that's like $200k per month to run ColBERT, like out of the box, non-optimized ColBERT. So search is not only about finding a good system, but always perfect trade off of good systems versus cost around the search. And here it's interesting to play around with like different parameters to say, okay, for which setting, which provides like the optimal trade off between cost and performance. And I think here finding generic answers is really hard to say. Okay, I dunno. ColBERT is always better or dense embeds is better or sparse is better. Because I think it can also depend where you are. Do you have like a small data set and one really high quality? Then ColBERT is good. If you have a massive data set and you're okay with somewhat good quality, then ColBERT is probably like way too expensive and you have to use other approaches like binary embeddings.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Yeah, there's a lot. I'm still reading this paper from, Christopher Potts and a big team, I just remember that name, but about the cost tradeoffs of like SPLADE, starting with BM25 is the cheapest thing, and then like SPLADE and then they have this PLAID engine behind ColBERT. But yeah, this general topic around the cost of the search systems is so interesting as we're releasing like DiskANN and the Vamana thing compared to HNSW that has the in-memory thing. And the product quantization of these things around making it run with say CPUs vs. GPUs. I'm also very interested in a company called Neural Magic that's doing sparsity for the model acceleration. That whole topic is so interesting and so maybe transitioning, the last time I heard you give this lecture, it was a really great lecture on Cohere talks. You had four key points and I kind of want to walk through them and I think this would be good, so from ColBERT, I want to talk about sparse vectors. So, I don't know if ColBERT is quite a sparse vector idea. It's kind of that idea where you have a representation of each token. So I kind of want to talk about like BM25, SPLADE, this idea that you have like a maske language modeling head that projects the token into a sparse representation. So can you tell me how you're thinking about sparsity these days?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "I like the SPLADE model. I'm a big fan of it. I really like it because it's really close to masked language modeling. So the issue with dense embedding models is, if you train BERT MLM, it gives you really bad dense embedding models out of the box, weaker than averaging word2vec. But this MLM objective in SPLADE is really close between like SPLADE and MLM. You can take a word, you'll see, okay, what are possible synonyms here? And then you put all these possible synonyms for every token into a sparse vector. So here you have a really close match between fine-tuning and pre-training, which is extremely nice. Also they have shown really strong results on out-of-domain. So a big challenge for dense embedding model is new words. So people create new things like, I dunno, ChatGPT. Pre-trained dense embedding models have no idea what it is, but like sparse models, they just project it to new dimensions. You can search for it. So as soon as you search for it, you find it, which solves a lot of issues. Topics which I'm really interested in, which they sadly also don't solve is like long document encoding. So SPLADE works on a paragraph level, so maybe up to a few hundred, but in most cases you have like really long documents, like thousands, tens of thousands of words. I dunno. In some industry manufacturing setting, you have PDF documents of 10,000 pages, how to repair some machine. And their the question how do you, can you encode these long documents? And sadly it doesn't work with SPLADE and also doesn't in principle work with sparse vectors.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "From guiding my thinking with these four things, I've kind of started to put long documents and multi discourse, kind of together into the same category where I mean, yeah, long documents is such an interesting problem. I think with the ChatGPT and like the dialogue models particularly, it's making a lot of people think about this. So here's maybe what I've been thinking about and I'm really curious what your thoughts are on this, is sort of clustering the windows, and then because you embed the windows of the long document and then you kind of cluster it and then you have sort of a representative centroid that you try to pack into the context window, sort of, and maybe also in addition to, you know, the retrieving facts and it's some kind of prompt designed to overcome the long document limitation.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yeah. So I think here you need to differentiate between these two things, like more ChatGPT, where you operate on like some history, and in terms of retrieval. So in terms of retrieval, the big challenge is kind of like co-references, so if you take the annual report from Apple, you just have like Apple on the front page of the annual report. So the annual report is, I dunno, 200 pages long. And then down the line on the paragraph level, it just says we, so instead of Apple, they just say, we. And you can't have a paragraph where they write, we saw a big increase in demand for all our products in this market.  And the other questions like what is we, what is our products and what is this market? And you as a human, you know, okay, it's the Apple annual report, here in this sub-section, I dunno, iPhone sales in North America. And then you can fill in the blanks, but the the question is, how do you fetch up this co-reference? Or how do you fetch up this information?  And the ChatGPT challenge here. What works so far is increasing the context lengths because, I don't know, if you start a blank session, it takes some time. So if it has like a context length of 8,000, so, so right now the best models can do like up to 8,000 word pieces. That's like eight pages. So it takes a time until you write like eight pages of tokenization with ChatGPT. So you can just do massive attention for, probably over a long time. If you really build up, I dunno, if you have like a lot of interaction with systems, we need to find ways to, to build up some type of memory. So we do it with humans, we know, this person, I don't know, has these hobbies, this work, this traveling to this location and then we can query this somehow and use this in our current context. So, either we have to find a way to build up some small memory which we can quickly, efficiently query what we have to have. So some query thing into, okay, on what topic is the person working? So that I can create a follow up question on this, but still that's, I don't know really. I think a really early field, how can the system quickly access memory and build on top of this to get new ideas from past history?",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Yeah, I mean, the whole external transformer memory idea is something that we're just crazy excited about with Weaviate and the vector index and all that. But, really quick, I want to go a little deeper into this co-reference resolution. It sounds like such a problem and I think communicates the idea well, so like three paragraphs or eight pages? Well, so in the context of retrieval models where say we can only take in like two paragraphs as input, so like four paragraphs into it, it said like, yeah, apple is the, and then we are, and then we use Apple. So does that mean we maybe need like a co-reference resolution parser to parse our text before it's vectorized? Or do we maybe need to create like a graph? Edges linking entities?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yeah, that's a good question. We're currently doing research on it. So it depends kind of on the dataset. So right now a lot of people work on Wikipedia, but Wikipedia is kind of boring because it's like really self-contained. So if you take a paragraph from Wikipedia, I dunno, you take a random paragraph from the Barack Obama article. It will mention Barack Obama, like every paragraph mentions Barack Obama, but in Annual reports they, I dunno, Apple will just say we. And then on the front page, you know it's Apple Annual reports, you go on page hundred, here's the section about, I dunno, iPhones and then, Page 150 is iPhone sales in North America. And then you have the paragraph, we saw a big increase for our products in this market. And so you kind of need to do like co-reference resolution across many pages. So potentially going back a hundred pages to know, okay, this the upper annual report from 2021. And there's the question like, how can we do this? Because then we come back to an old problem. So right now all people in co-reference resolution are mainly using transformer networks, which are again limited to, I dunno, 512 word pieces. So, but how can we do this co reference resolution, possibly over hundreds of pages and also a really complex setting so that we really don't know how to do it. And yeah, I think that the best way is if we chunk the paragraphs and encode them, is to really re-write the paragraph so that if you just see the paragraph and nothing else, you have all the relevant context information. So if I rewrite, like, we saw big increase in our products in this market and the last year and rewrite it to, Apple saw a lot increase in demand for the iPhone in the North America market in 2020. Then it's nice, you can find it for a lot of such questions. But yeah, open questions like how do you do this rewriting, especially, this requires again, to resolve long context co-reference resolutions.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Yeah, that's so interesting. I think of that as a data augmentation, maybe more than a pre-processing because you're kind of trying to encode the invariance of the original paragraph without the co-references resolved. And then the new kind of, instead of we, it's Apple and then you're trying to say, be invariant to these two transformations. And I think it's very interesting as the problem eats itself, like the co-reference resolution models are also limited to the context. So yeah, that's a pretty interesting problem.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "And then obviously we can also change, I don't know, maybe they have like some subdivision and then we can mean, I don't know, not only Apple, but maybe, I don't know, Apple Europe or so. Or some you have like some parent company and then this is like some child companies, so, so we can refer to the child company, or can I refer to the whole group or to some product teams?",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Have you thought about like entity parsing, entity extraction? It's a little different from the sparse search wiith the hybrid. The BM25 is, if the query is, How to catch an Alaskan Pollock, it'll be emphasizing the Alaskan Pollock sort of with the keyword matching thing. But there's also this idea of like entity extraction, Alaskan Pollock, and maybe it's that multi-vector ranking where you also search with Alaskan Pollock.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yeah, I mean the research is still really early in this field, so the big limitation is so far datasets. A lot of data sets are built on Wikipedia, which does not really require to take a lot of context into account. Then the question, is it just entities or is it also other things that are more than entities? So that's also an open research question. Is it just fine to to resolve like we and this and last quarter, or do we have to also resolve other things and yeah, in my experience, often in these examples you make up like, I dunno, we saw big demand. It's kind of easy, but then if you go to actual sentences, you say, okay, it's a lot more nuanced. Which context information do you still need to understand this paragraph? Maybe context information can also map to a table with financial data. It can map to some graph, and then the questions, okay, maybe they have some pie chart and they're discussing the pie chart. How can you contextualize the pie chart into this paragraph so that if you just include the paragraph, have all the necessary information and without running out of context lengths or word lengths again.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "Yeah, that's another really interesting topic. I think there's so many things to talk about, but like that idea of the tables and the graphs and getting that into the text domain. It kind of reminds me of the same data augmentation pre-processing we discussed with the co-reference resolution where you can parse the tables. I've seen this paper on, it's like learning to reason across wiki tables, some title like that where they're parsing out the tables to go from table structure to text structure. And then because you kind of unify the domains without having to have like a visual component to it. Yeah, anyway. So another topic, kind of a finishing on the four things is the topic of unknown words and distribution shift, maybe actually those are kind of two topics. Maybe we'll start with distribution shift because it's so exciting. So I recently read this paper that was really interesting called OOD-DiskANN, where they're describing the distribution shift in if you build up an image index and then you're searching with the text queries, they're going to be out of distribution for the HNSW, or the proximity graph structure and that kind of idea. So how are you thinking about distribution shift these days?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Yeah, so distribution shifts are interesting on multiple aspects. So language evolves. So that's a pain point of language or a nice feature of language. You see that things evolving in meaning. So I dunno, like Corona before 2020, it was like mainly beer, then sadly in 2020, if someone talks about Corona, it's connected to a virus. Similar. If you talk about Omnicron, it used to be some Chinese and some Greek characters maybe relevant for mathematical equations. Now people talk about there's like a new omnicron wave in, in Europe or stuff like that, or there's a new omnicron variant. It's like extremely hard to really update the models here. And a big challenge is also in terms of first stage and second stage retreival. So if you're taking an embedding model that has like no idea about Coronavirus. Maybe on new training data you can update the model that Corona now has a new meaning. But then you come back to needing to think about, do you need to re-encode your whole corpus? So if you do, you have to take the embedding model and re-encode like the whole corpus. If your corpus are small, like a million documents, it's okay. What if your corpus is the web, like Google or Bing? You have to re-encode like all documents on the web just to create like the new embeddings. And so there's like an interesting thing or question like, okay, can you make this bit more efficient? So, so which can you kind of like transform the embedding? So do, can you do like some partial updates? So you can just update that Corona now has two meanings, the beer and the virus, and can you find like the documents you wanna update, but everything else you can keep the same. Be able to decide when do you do the updates or when do you train again, this new embedding model and so on. So many open unsolved research questions here right now. People do like the most stupid thing. They completely update the embedding model and then encode the whole corpus again. Even such that 99.9 percent are completely touched from the update. So exciting research questions to know, okay, what do you need to update and how can you update this? And also they're the interplay between first and second stage. So maybe it's okay to have like an outdated first stage retrieval that does not really know what is Corona, but then have like a more recent up to date, second stage retrieval that Corona can also mean a virus now. And not only, Yeah.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "What I thought you just said was incredibly inspiring. Because there's this argument about, with the whole, Weaviate-augmented GPT thing where it's like, You know, the big language model doesn't need, it can be updated for these new facts by retrieving it. But then it's like, well what about the retrieval model? And then there's that problem of you retrain the retrieval model on the new information, like the new omnicron variant. And then you have to re-encode the whole corpus. And if you have like, you know, hundreds of millions or whatever, and we thought about this idea that is kinda interesting that I'd like to pitch to you as well like maybe we could use that proximity graph structure as a prior to try to propagate the representation space changes and that way we could maybe only re-vectorize like 10,000 documents. And they could be kind of cleverly selected with their centrality in the proximity graph. And then we could maybe have some sort of graph neural network, that can propagate representation changes. I dunno if this is too grandiose of an idea, What do you think about that idea?",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "I think there you quickly run in the problem or the challenge to know what has changed. So let's say what has changed in 2022 and then maybe for some top things you can name, but it's like really hard in the long tail to say, okay, what has changed? Like are there new, in some new online games, popular online game, is there like some new patch which introduces new characters, which have like a new acronym and now everyone refers to them with like a different acronym or so. So I think that's like the challenge like really what has changed between now and the year before and to really enumerate this to not only say, okay, not only, I dunno, Corona change the meaning, but really in the long tail, what has all changed the meaning and where do you all need to do the update? I think that's the biggest challenge, how to automate this and how to find it. Also in terms of preferences, if I search for how can I get Corona in theory, how I catch the virus and how I catch or how I buy the beer should both be relevant. So if you ask an annotator, they should both be annotated as relevant. Probably if you see a search in Corona and people say, how do I get Corona? You probably want to rank the virus higher than the beer and then maybe, hopefully in a few years you wanna rank the beer high again. But there also, the question, like how do you build up this popularity? So it's not only relevancy in search, but also popularity of terms. Like, NFT popped up at some point for non fungible tokens. So if someone searched for nft, we're meaning, non fungible tokens. But there were meanings for NFT before, so there's a book series, not for tourist, also appreciated with nft. So how can you learn? Okay, there's a new meaning, which is a lot more popular. Also crypto. I started myself to work on cryptography. So crypto, 10 years ago you referred to cryptography, then Bitcoin and so on happened. And now if you talk about crypto, everyone talks about like Bitcoin and stuff.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
        "content": "I think, well, yeah, this idea of kind of updating the unknown words, it sounds pretty difficult and I don't even have anything to kinda propose, but that's kind of why I also really like this intent stating as well in the search engine interface where, how to catch Corona, nd then maybe if you just said your intent, like, I'm looking for a beer. It's like a different way to search and it's more verbose than usual. But if it gives you better results, maybe, especially with he kind of idea that search interfaces are moving into generating you a long answer instead of just necessarily returning documents. And maybe you'd be willing to put more effort into your query. I think generally the search interface thing is evolving. I want to ask you that question about search interfaces really quick.",
        "sender": "Connor Shorten",
        "channelNum": 33
    },{
        "content": "Just a second one on this intent. I mean, it's kind of old idea. I dunno, before that it was named query re-writing. I mean we as humans do it. If I search for Corona, I get the wrong results. I search for Corona beers, so we are already trained to do this. I search for, how to sort a list and I forgot which programming language I add, like Python to it. And then also in the back end, even when Google was created was smartly adding query rewriting to it. So when I look for like an Italian restaurant, it was adding smartly in the background, the current location I'm in, and we're looking for like Italian restaurant in Darmstadt. So it's kind of an old technique. This query rewriting, adding, trying to guess the intent.",
        "sender": "Nils Reimers",
        "channelNum": 33
    },{
       "content": "Yeah, that's super cool, I saw the learning to reformulate queries with reinforcement learning where the action space are the keywords and then you can add them, subtract them, boost the importance of them. Yeah, it's super cool. I guess I'm kind of losing interest in that idea due to the challenge of maybe a reinforcement learning query writer. It sounds like a lot of overhead in the search pipeline. Yeah, so interesting. So anyways, Nils, thank you so much for joining the Weaviate podcast. I think we covered so many interesting topics. I'm really looking forward to rewatching this and rethinking about all these ideas. Thanks so much for your contributions to search broadly and I'm really loving these Cohere talks you're giving. I think you're doing another one in about an hour and so I'll see you over there and I'm really looking forward to it. And thanks again.",
       "sender": "Connor Shorten",
       "channelNum": 33
    },{
       "content": "Great. Thank you so much for being here. Bye-bye.",
       "sender": "Nils Reimers",
       "channelNum": 33
    }
]
